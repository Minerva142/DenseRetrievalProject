{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-16T18:41:07.288295Z",
     "iopub.status.busy": "2025-01-16T18:41:07.287983Z",
     "iopub.status.idle": "2025-01-16T18:41:30.758521Z",
     "shell.execute_reply": "2025-01-16T18:41:30.757527Z",
     "shell.execute_reply.started": "2025-01-16T18:41:07.288269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install -U sentence-transformers\n",
    "!pip install pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T18:41:30.760286Z",
     "iopub.status.busy": "2025-01-16T18:41:30.759940Z",
     "iopub.status.idle": "2025-01-16T18:41:52.884969Z",
     "shell.execute_reply": "2025-01-16T18:41:52.884087Z",
     "shell.execute_reply.started": "2025-01-16T18:41:30.760247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import pytrec_eval\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T18:41:52.886943Z",
     "iopub.status.busy": "2025-01-16T18:41:52.886412Z",
     "iopub.status.idle": "2025-01-16T18:41:52.895627Z",
     "shell.execute_reply": "2025-01-16T18:41:52.894726Z",
     "shell.execute_reply.started": "2025-01-16T18:41:52.886917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve(query_embeddings, top_k=10):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar documents for multiple queries\n",
    "    \n",
    "    :param query_embeddings: Numpy array of query embeddings\n",
    "    :param top_k: Number of documents to retrieve\n",
    "    :return: Dictionary of results for pytrec_eval\n",
    "    \"\"\"\n",
    "    # Ensure query embeddings are 2D\n",
    "    if query_embeddings.ndim == 1:\n",
    "        query_embeddings = query_embeddings.reshape(1, -1)\n",
    "    \n",
    "    # Search index\n",
    "    distances, indices = index.search(query_embeddings, top_k)\n",
    "    \n",
    "    # Convert results to dictionary format for pytrec_eval\n",
    "    results = {}\n",
    "    for i, (doc_indices, doc_distances) in enumerate(zip(indices, distances)):\n",
    "        # Use query index as string key\n",
    "        query_key = str(i)\n",
    "        results[query_key] = {\n",
    "            docnos[idx]: dist  # Use docnos instead of ids\n",
    "            for idx, dist in zip(doc_indices, doc_distances) \n",
    "            if idx != -1\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def load_qrels(qrels_path):\n",
    "    \"\"\"\n",
    "    Load relevance judgments from TREC qrels file\n",
    "    \n",
    "    :param qrels_path: Path to qrels file\n",
    "    :return: Dictionary of relevance judgments\n",
    "    \"\"\"\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Typical TREC qrels format: query_id 0 doc_id relevance\n",
    "            query_id, _, doc_id, relevance = line.strip().split()\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    return qrels\n",
    "\n",
    "def load_queries(queries_file):\n",
    "    \"\"\"\n",
    "    Load queries from a file\n",
    "    \n",
    "    :param queries_file: Path to queries JSON file\n",
    "    :return: Tuple of (query_texts, query_ids)\n",
    "    \"\"\"\n",
    "    with open(queries_file, 'r') as f:\n",
    "        queries_data = json.load(f)\n",
    "    \n",
    "    # Assuming JSON structure with 'text' and 'id' fields\n",
    "    query_texts = [query.get('text', '') for query in queries_data]\n",
    "    query_ids = [query.get('id', str(idx)) for idx, query in enumerate(queries_data)]\n",
    "\n",
    "    return query_texts, query_ids\n",
    "\n",
    "def compute_aggregated_measures(metrics):\n",
    "    \"\"\"\n",
    "    Manually compute aggregated measures across queries\n",
    "    \n",
    "    :param metrics: Dictionary of per-query metrics from pytrec_eval\n",
    "    :return: Dictionary of aggregated metrics\n",
    "    \"\"\"\n",
    "    aggregated_metrics = {}\n",
    "    \n",
    "    # Metrics to aggregate\n",
    "    metric_keys = [\n",
    "        'ndcg', 'map', 'recip_rank', \n",
    "        'P_5', 'P_10', 'P_20', \n",
    "        'recall_5', 'recall_10', 'recall_20'\n",
    "    ]\n",
    "    \n",
    "    for metric in metric_keys:\n",
    "        # Collect all values for this metric\n",
    "        metric_values = []\n",
    "        for query_metrics in metrics.values():\n",
    "            if metric in query_metrics:\n",
    "                metric_values.append(query_metrics[metric])\n",
    "        \n",
    "        # Compute aggregation methods\n",
    "        if metric_values:\n",
    "            aggregated_metrics[f'{metric}_mean'] = np.mean(metric_values)\n",
    "            aggregated_metrics[f'{metric}_median'] = np.median(metric_values)\n",
    "            aggregated_metrics[f'{metric}_std'] = np.std(metric_values)\n",
    "    \n",
    "    return aggregated_metrics\n",
    "\n",
    "def encode_document(doc):\n",
    "    # Generate embeddings for the sentences\n",
    "    embeddings = model.encode(doc)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def normalize_vectors(vectors):\n",
    "    \"\"\"\n",
    "    Normalize vectors to unit length\n",
    "    \"\"\"\n",
    "    return normalize(vectors, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T19:11:14.250189Z",
     "iopub.status.busy": "2025-01-16T19:11:14.249789Z",
     "iopub.status.idle": "2025-01-16T19:17:58.163571Z",
     "shell.execute_reply": "2025-01-16T19:17:58.162811Z",
     "shell.execute_reply.started": "2025-01-16T19:11:14.250159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of models to use\n",
    "models_to_evaluate = [\n",
    "    'multi-qa-distilbert-dot-v1',\n",
    "    'multi-qa-MiniLM-L6-dot-v1',\n",
    "    'multi-qa-mpnet-base-cos-v1',\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'all-MiniLM-L12-v2',\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'multi-qa-distilbert-cos-v1',\n",
    "    'multi-qa-MiniLM-L6-cos-v1',\n",
    "    'multi-qa-mpnet-base-dot-v1',\n",
    "    'distiluse-base-multilingual-cased-v1',\n",
    "    'distiluse-base-multilingual-cased-v2',\n",
    "]\n",
    "query_texts, query_ids = load_queries('/kaggle/input/query-and-qrels/queries.json')\n",
    "\n",
    "qrels = load_qrels('/kaggle/input/query-and-qrels/filtered_data.txt')\n",
    "\n",
    "\n",
    "# Dictionary to store results for all models\n",
    "all_results = {}\n",
    "\n",
    "def create_faiss_index(embeddings, model_name):\n",
    "    \"\"\"\n",
    "    Create appropriate FAISS index based on model type\n",
    "    \"\"\"\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Check if model is using cosine similarity ('cos' in name)\n",
    "    #if 'cos' in model_name.lower():\n",
    "    # For cosine similarity, normalize vectors and use IP distance\n",
    "    normalized_embeddings = normalize_vectors(embeddings)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(normalized_embeddings)\n",
    "    #else:\n",
    "        # For dot product or L2 models, use raw vectors with L2 distance\n",
    "    #index = faiss.IndexFlatL2(dim)\n",
    "    #index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    # Load embeddings for current model\n",
    "    embeddings_file = f'/kaggle/input/{model_name.lower()}-ft-embeddings/{model_name}_FT_embeddings.csv'\n",
    "    df = pd.read_csv(embeddings_file)\n",
    "    \n",
    "    # Extract document embeddings\n",
    "    docnos = df['DOCNO'].values\n",
    "    embeddings = df.drop(columns=['DOCNO']).values\n",
    "    \n",
    "    # Create appropriate index based on model type\n",
    "    index = create_faiss_index(embeddings, model_name)\n",
    "    \n",
    "    # Load model and encode queries\n",
    "    model = SentenceTransformer(f'sentence-transformers/{model_name}')\n",
    "    query_embeddings = model.encode(query_texts)\n",
    "    \n",
    "    #Normalize queries only for cosine similarity models\n",
    "    #if 'cos' in model_name.lower():\n",
    "    query_embeddings = normalize_vectors(query_embeddings)\n",
    "    \n",
    "    # Retrieve results\n",
    "    run = retrieve(query_embeddings)\n",
    "    run_with_query_ids = {\n",
    "        query_ids[int(k)]: v for k, v in run.items()\n",
    "    }\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    corrected_version = {\n",
    "        str(query_id): {str(doc_id): float(score) for doc_id, score in doc_scores.items()}\n",
    "        for query_id, doc_scores in run_with_query_ids.items()\n",
    "    }\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrels,\n",
    "        {\n",
    "            'ndcg', 'map', 'recip_rank',\n",
    "            'P_5', 'P_10',\n",
    "            'recall_5', 'recall_10', \n",
    "        }\n",
    "    )\n",
    "    \n",
    "    metrics = evaluator.evaluate(corrected_version)\n",
    "    aggregated_measures = compute_aggregated_measures(metrics)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[model_name] = aggregated_measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T19:17:58.164958Z",
     "iopub.status.busy": "2025-01-16T19:17:58.164641Z",
     "iopub.status.idle": "2025-01-16T19:17:58.186871Z",
     "shell.execute_reply": "2025-01-16T19:17:58.186175Z",
     "shell.execute_reply.started": "2025-01-16T19:17:58.164934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sort models by NDCG score\n",
    "ndcg_scores = {model: results['ndcg_mean'] for model, results in all_results.items()}\n",
    "sorted_models = dict(sorted(ndcg_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Create sorted results dictionary\n",
    "sorted_results = {model: all_results[model] for model in sorted_models.keys()}\n",
    "\n",
    "# Create and save sorted DataFrame\n",
    "results_df = pd.DataFrame.from_dict(sorted_results, orient='index')\n",
    "results_df.to_csv('model_evaluation_results.csv')\n",
    "\n",
    "# Display ranking\n",
    "print(\"\\nModel Ranking by NDCG (Normalized Discounted Cumulative Gain):\")\n",
    "print(\"-\" * 50)\n",
    "for rank, (model, ndcg_score) in enumerate(sorted_models.items(), 1):\n",
    "    print(f\"{rank}. {model:<35} NDCG: {ndcg_score:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Results (sorted by NDCG):\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6310116,
     "sourceId": 10209807,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6374102,
     "sourceId": 10298181,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6377327,
     "sourceId": 10302942,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6489812,
     "sourceId": 10480754,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6489904,
     "sourceId": 10480907,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6489979,
     "sourceId": 10481114,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6489987,
     "sourceId": 10481129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6492669,
     "sourceId": 10486289,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6492891,
     "sourceId": 10486670,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6492956,
     "sourceId": 10486817,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6493001,
     "sourceId": 10486881,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6494835,
     "sourceId": 10489818,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6495153,
     "sourceId": 10490244,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
