{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 9982294,
     "sourceType": "datasetVersion",
     "datasetId": 6142619
    },
    {
     "sourceId": 10096745,
     "sourceType": "datasetVersion",
     "datasetId": 6226809
    },
    {
     "sourceId": 10113929,
     "sourceType": "datasetVersion",
     "datasetId": 6239976
    },
    {
     "sourceId": 10209807,
     "sourceType": "datasetVersion",
     "datasetId": 6310116
    },
    {
     "sourceId": 120002,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 100933,
     "modelId": 121027
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install faiss-gpu\n!pip install pytrec_eval",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:33:44.537931Z",
     "iopub.execute_input": "2024-12-24T14:33:44.538167Z",
     "iopub.status.idle": "2024-12-24T14:34:06.355033Z",
     "shell.execute_reply.started": "2024-12-24T14:33:44.538141Z",
     "shell.execute_reply": "2024-12-24T14:34:06.353991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.5/85.5 MB\u001B[0m \u001B[31m19.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nCollecting pytrec_eval\n  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\nBuilding wheels for collected packages: pytrec_eval\n  Building wheel for pytrec_eval (setup.py) ... \u001B[?25l\u001B[?25hdone\n  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp310-cp310-linux_x86_64.whl size=308217 sha256=dc461848bb450ae3cb552b29ce926e0da910cb355b419fd61117077a26813212\n  Stored in directory: /root/.cache/pip/wheels/51/3a/cd/dcc1ddfc763987d5cb237165d8ac249aa98a23ab90f67317a8\nSuccessfully built pytrec_eval\nInstalling collected packages: pytrec_eval\nSuccessfully installed pytrec_eval-0.5\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport numpy as np\nimport faiss\nimport json\nimport os\nimport pytrec_eval",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:34:32.071784Z",
     "iopub.execute_input": "2024-12-24T14:34:32.072118Z",
     "iopub.status.idle": "2024-12-24T14:34:35.958742Z",
     "shell.execute_reply.started": "2024-12-24T14:34:32.072089Z",
     "shell.execute_reply": "2024-12-24T14:34:35.958116Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# Set the environment variable for PyTorch CUDA memory allocation\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Check if GPU is available and set device accordingly\nprint(torch.cuda.is_available())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\n# Load the tokenizer and model\nmodel_path = \"/kaggle/input/llama-3.2/transformers/1b-instruct/1\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModel.from_pretrained(model_path)\n\n\n# Extract the embedding layer\nembedding_layer = model.get_input_embeddings()  # works for LLama 3.2\ndel model\n\nembedding_layer.to(device)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:34:38.325138Z",
     "iopub.execute_input": "2024-12-24T14:34:38.325549Z",
     "iopub.status.idle": "2024-12-24T14:35:01.294402Z",
     "shell.execute_reply.started": "2024-12-24T14:34:38.325527Z",
     "shell.execute_reply": "2024-12-24T14:35:01.293467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "True\n",
     "output_type": "stream"
    },
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "Embedding(128256, 2048)"
     },
     "metadata": {}
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "# Encode documents into dense vectors\ndef encode_documents(documents):\n    inputs = tokenizer(documents, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        embeddings = embedding_layer(inputs['input_ids']).mean(dim=1) # Mean pooling on GPU\n        embeddings /= embeddings.norm(dim=1, keepdim=True) # Normalize embeddings\n\n    del inputs\n    return embeddings.cpu().numpy()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:35:07.414410Z",
     "iopub.execute_input": "2024-12-24T14:35:07.414720Z",
     "iopub.status.idle": "2024-12-24T14:35:07.419550Z",
     "shell.execute_reply.started": "2024-12-24T14:35:07.414697Z",
     "shell.execute_reply": "2024-12-24T14:35:07.418815Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "# original dataset was loaded to output target document after the seach\n\n# Path to your input JSON file\npath_to_json = '/kaggle/input/merget-times/merged_output.json'\n\n# Load your JSON data from a file\nwith open(path_to_json, 'r') as file:\n    data = json.load(file)\n\n# Create a dictionary where DOCNO is the key and TEXT is the value\ndocuments = {}\nfor entry in data:\n    key = entry[\"DOCNO\"]\n    value = entry[\"TEXT\"]\n    documents[key] = value\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:35:09.484698Z",
     "iopub.execute_input": "2024-12-24T14:35:09.485076Z",
     "iopub.status.idle": "2024-12-24T14:35:16.126824Z",
     "shell.execute_reply.started": "2024-12-24T14:35:09.485036Z",
     "shell.execute_reply": "2024-12-24T14:35:16.126098Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "import csv\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load dense vectors from CSV file\ndef load_vectors_from_csv(file_path):\n    ids = []\n    vectors = []\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        next(reader)  # Skip the header row if it exists\n        for row in reader:\n            ids.append(row[0])  # Assuming the first column is IDs\n            vectors.append([float(value) for value in row[1:]])  # Remaining columns are vector values\n    vectors = np.array(vectors, dtype=np.float32)\n    return ids, vectors\n\n# Load dense vectors\nids, doc_vectors = load_vectors_from_csv('/kaggle/input/financial-timel-llama3-2-1b-instruct-dense-vectors/document_embeddings_Llama3_2_1b_instruct.csv')\n\n# Move document vectors to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndoc_vectors_tensor = torch.tensor(doc_vectors).to(device)\n\n# Create FAISS index for efficient similarity search (cosine similarity)\ngpu_res = faiss.StandardGpuResources()  # Create resources for managing GPU memory\nindex = faiss.IndexFlatIP(doc_vectors_tensor.shape[1])  # Inner product index (for cosine similarity)\n\n# Transfer the index to GPU and add document vectors directly\ngpu_index = faiss.index_cpu_to_gpu(gpu_res, 0, index)  # Transfer index to GPU\ngpu_index.add(doc_vectors_tensor.cpu().numpy())  # Add document vectors (needs numpy array)\n\n",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:35:20.787838Z",
     "iopub.execute_input": "2024-12-24T14:35:20.788189Z",
     "iopub.status.idle": "2024-12-24T14:38:21.788684Z",
     "shell.execute_reply.started": "2024-12-24T14:35:20.788161Z",
     "shell.execute_reply": "2024-12-24T14:38:21.787954Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "# Function to perform a search query using GPU index\ndef search(query, k=5):\n    \n    query_vector = encode_documents(query)\n    \n    # Search using FAISS index directly on GPU\n    distances, indices = gpu_index.search(query_vector, k)  \n    \n    return indices[0], distances[0]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:41:52.299430Z",
     "iopub.execute_input": "2024-12-24T14:41:52.299739Z",
     "iopub.status.idle": "2024-12-24T14:41:52.304128Z",
     "shell.execute_reply.started": "2024-12-24T14:41:52.299716Z",
     "shell.execute_reply": "2024-12-24T14:41:52.303120Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "def load_qrels(qrels_path):\n    \"\"\"\n    Load relevance judgments from TREC qrels file\n    \n    :param qrels_path: Path to qrels file\n    :return: Dictionary of relevance judgments\n    \"\"\"\n    qrels = {}\n    with open(qrels_path, 'r') as f:\n        for line in f:\n            # Typical TREC qrels format: query_id 0 doc_id relevance\n            query_id, _, doc_id, relevance = line.strip().split()\n            if query_id not in qrels:\n                qrels[query_id] = {}\n            qrels[query_id][doc_id] = int(relevance)\n    return qrels\n\ndef load_queries(queries_file):\n    \"\"\"\n    Load queries from a file\n    \n    :param queries_file: Path to queries JSON file\n    :return: Tuple of (query_texts, query_ids)\n    \"\"\"\n    with open(queries_file, 'r') as f:\n        queries_data = json.load(f)\n    \n    # Assuming JSON structure with 'text' and 'id' fields\n    query_texts = [query.get('text', '') for query in queries_data]\n    query_ids = [query.get('id', str(idx)) for idx, query in enumerate(queries_data)]\n\n    return query_texts, query_ids",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:41:54.587804Z",
     "iopub.execute_input": "2024-12-24T14:41:54.588107Z",
     "iopub.status.idle": "2024-12-24T14:41:54.593701Z",
     "shell.execute_reply.started": "2024-12-24T14:41:54.588082Z",
     "shell.execute_reply": "2024-12-24T14:41:54.592760Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "def retrieve(query_embeddings, top_k=10):\n",
    "        \"\"\"\n",
    "        Retrieve top-k most similar documents for multiple queries\n",
    "        \n",
    "        :param query_embeddings: Numpy array of query embeddings\n",
    "        :param top_k: Number of documents to retrieve\n",
    "        :return: Dictionary of results for pytrec_eval\n",
    "        \"\"\"\n",
    "        # Ensure query embeddings are 2D\n",
    "        if query_embeddings.ndim == 1:\n",
    "            query_embeddings = query_embeddings.reshape(1, -1)\n",
    "        \n",
    "        # Search index\n",
    "        distances, indices = gpu_index.search(query_embeddings, top_k)\n",
    "        \n",
    "        # Convert results to dictionary format for pytrec_eval\n",
    "        results = {}\n",
    "        for i, (doc_indices, doc_distances) in enumerate(zip(indices, distances)):\n",
    "            # Use query index as string key\n",
    "            query_key = str(i)\n",
    "            results[query_key] = {\n",
    "                #ids[idx]: float(1 / (1 + dist)) \n",
    "                ids[idx]: dist\n",
    "                for idx, dist in zip(doc_indices, doc_distances) \n",
    "                if idx != -1\n",
    "            }\n",
    "        \n",
    "        return results"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:41:57.187622Z",
     "iopub.execute_input": "2024-12-24T14:41:57.188098Z",
     "iopub.status.idle": "2024-12-24T14:41:57.193154Z",
     "shell.execute_reply.started": "2024-12-24T14:41:57.188062Z",
     "shell.execute_reply": "2024-12-24T14:41:57.192241Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "def compute_aggregated_measures(metrics):\n    \"\"\"\n    Manually compute aggregated measures across queries\n    \n    :param metrics: Dictionary of per-query metrics from pytrec_eval\n    :return: Dictionary of aggregated metrics\n    \"\"\"\n    aggregated_metrics = {}\n    \n    # Metrics to aggregate\n    metric_keys = [\n        'ndcg', 'map', 'recip_rank', \n        'P_5', 'P_10', 'P_20', \n        'recall_5', 'recall_10', 'recall_20'\n    ]\n    \n    for metric in metric_keys:\n        # Collect all values for this metric\n        metric_values = []\n        for query_metrics in metrics.values():\n            if metric in query_metrics:\n                metric_values.append(query_metrics[metric])\n        \n        # Compute aggregation methods\n        if metric_values:\n            aggregated_metrics[f'{metric}_mean'] = np.mean(metric_values)\n            aggregated_metrics[f'{metric}_median'] = np.median(metric_values)\n            aggregated_metrics[f'{metric}_std'] = np.std(metric_values)\n    \n    return aggregated_metrics",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T14:41:59.844973Z",
     "iopub.execute_input": "2024-12-24T14:41:59.845341Z",
     "iopub.status.idle": "2024-12-24T14:41:59.850507Z",
     "shell.execute_reply.started": "2024-12-24T14:41:59.845313Z",
     "shell.execute_reply": "2024-12-24T14:41:59.849535Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "query_texts, query_ids = load_queries('/kaggle/input/query-and-qrels/queries.json')\n",
    "\n",
    "qrels = load_qrels('/kaggle/input/query-and-qrels/filtered_data.txt')\n",
    "\n",
    "query_embeddings = encode_documents(query_texts)\n",
    "\n",
    "run = retrieve(query_embeddings)\n",
    "\n",
    "#print(run)\n",
    "\n",
    "run_with_query_ids = {\n",
    "        query_ids[int(k)]: v for k, v in run.items()\n",
    "    }\n",
    "\n",
    "\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrels, \n",
    "        {\n",
    "            'ndcg', 'map', 'recip_rank', \n",
    "            'P_5', 'P_10', 'P_20', \n",
    "            'recall_5', 'recall_10', 'recall_20'\n",
    "        }\n",
    "    )\n",
    "\n",
    "corrected_version = {\n",
    "    str(query_id): {str(doc_id): 1 for doc_id, score in doc_scores.items()}\n",
    "    for query_id, doc_scores in run_with_query_ids.items()\n",
    "}\n",
    "\n",
    "#print(run_with_query_ids)\n",
    "\n",
    "metrics = evaluator.evaluate(corrected_version)\n",
    "\n",
    "print(\"Aggregated Metrics:\")\n",
    "aggregated_measures = compute_aggregated_measures(\n",
    "        metrics\n",
    "    )\n",
    "\n",
    "for metric, value in sorted(aggregated_measures.items()):\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-24T15:02:30.668908Z",
     "iopub.execute_input": "2024-12-24T15:02:30.669363Z",
     "iopub.status.idle": "2024-12-24T15:02:30.791594Z",
     "shell.execute_reply.started": "2024-12-24T15:02:30.669321Z",
     "shell.execute_reply": "2024-12-24T15:02:30.790658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Aggregated Metrics:\nP_10_mean: 0.006711409395973154\nP_10_median: 0.0\nP_10_std: 0.02990910061094925\nP_20_mean: 0.003355704697986577\nP_20_median: 0.0\nP_20_std: 0.014954550305474625\nP_5_mean: 0.013422818791946308\nP_5_median: 0.0\nP_5_std: 0.0598182012218985\nmap_mean: 0.004852357827458591\nmap_median: 0.0\nmap_std: 0.042137814799734566\nndcg_mean: 0.008454156342145521\nndcg_median: 0.0\nndcg_std: 0.056357643355608514\nrecall_10_mean: 0.005572675687898926\nrecall_10_median: 0.0\nrecall_10_std: 0.04271952271257743\nrecall_20_mean: 0.005572675687898926\nrecall_20_median: 0.0\nrecall_20_std: 0.04271952271257743\nrecall_5_mean: 0.005572675687898926\nrecall_5_median: 0.0\nrecall_5_std: 0.04271952271257743\nrecip_rank_mean: 0.030425055928411632\nrecip_rank_median: 0.0\nrecip_rank_std: 0.1507961909801089\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": "# Example search query\nqueries = [\"Which bank decrease the mortage rate 11.5 per cent to 10.95\", #FT921-11403\n          \"Who is the minister of international economy in hungary at 1990\", #FT911-226\n          \"The situation and potential of Turkish/Turkey banks\", #FT922-6909\n           \"Who is the other bank that cooperates with Austrian Airlines in Europe?\", #FT922-6946\n           \"managing director of Renison Goldfields in 1992\", #FT923-13976\n           \"who will become to chief execute London-quoted\",#FT923-14206\n           \"how many people expected to on careers services at 92 march\",#FT923-14215\n           \"How does Lawler define subsidiarity, and how is it applied in high-involvement organizations\",#FT924-12138\n           \"What alternative solution does M. C. Kennedy propose to manage the fiscal situation without increasing taxes\",#FT931-7937\n           \"How do international bond funds differ from gilt unit trusts in terms of risks and returns\" #FT931-8107\n          ]\n\nexacts = [\"FT921-11403\", \"FT911-226\", \"FT922-6909\", \"FT922-6946\", \n          \"FT923-13976\", \"FT923-14206\", \"FT923-14215\", \"FT924-12138\", \"FT931-7937\", \"FT931-8107\"]\n\ni = 0\nfor query in queries:\n    print(query)\n    print(exacts[i])\n    indices, distances = search(query, 5)\n    #print(indices)\n    #print(distances)\n    # Retrieve and display results safely\n    for idx in range(len(indices)):\n        print(f\"Document: {ids[indices[idx]]} Distance: {distances[idx]}\")\n        #ind = ids[indices[idx]].replace(\" \", \"\")\n        if ids[indices[idx]] == exacts[i]:\n            print(\"Exact document found in ranked document list\")\n        # the code below also prints document content\n        # print(f\"Document: {ids[indices[idx]]} Distance: {distances[idx]} content: {documents[ids[indices[idx]]]}\")  \n    i += 1\n    \n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-15T21:22:51.182826Z",
     "iopub.execute_input": "2024-12-15T21:22:51.183479Z",
     "iopub.status.idle": "2024-12-15T21:22:51.238556Z",
     "shell.execute_reply.started": "2024-12-15T21:22:51.183431Z",
     "shell.execute_reply": "2024-12-15T21:22:51.237627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Which bank decrease the mortage rate 11.5 per cent to 10.95\nFT921-11403\nDocument: FT932-14632 Distance: 0.8512439131736755\nDocument: FT933-10336 Distance: 0.850742757320404\nDocument: FT933-8586 Distance: 0.849536120891571\nDocument: FT943-9027 Distance: 0.8494161367416382\nDocument: FT933-12337 Distance: 0.8490382432937622\nWho is the minister of international economy in hungary at 1990\nFT911-226\nDocument: FT932-3841 Distance: 0.7925577759742737\nDocument: FT942-526 Distance: 0.7920131683349609\nDocument: FT934-13423 Distance: 0.7915689945220947\nDocument: FT944-8638 Distance: 0.7912970185279846\nDocument: FT941-7543 Distance: 0.7905198931694031\nThe situation and potential of Turkish/Turkey banks\nFT922-6909\nDocument: FT943-95 Distance: 0.5412183403968811\nDocument: FT932-2641 Distance: 0.5361122488975525\nDocument: FT922-2836 Distance: 0.5327141284942627\nDocument: FT941-10738 Distance: 0.5316784977912903\nDocument: FT921-9052 Distance: 0.5314403772354126\nWho is the other bank that cooperates with Austrian Airlines in Europe?\nFT922-6946\nDocument: FT944-6798 Distance: 0.7143533825874329\nDocument: FT934-11659 Distance: 0.7130494713783264\nDocument: FT942-6907 Distance: 0.7108776569366455\nDocument: FT921-10231 Distance: 0.7106636166572571\nDocument: FT923-6924 Distance: 0.7106209993362427\nmanaging director of Renison Goldfields in 1992\nFT923-13976\nDocument: FT921-8586 Distance: 0.6215910911560059\nDocument: FT933-13229 Distance: 0.6121893525123596\nDocument: FT934-3850 Distance: 0.6098403334617615\nDocument: FT942-8125 Distance: 0.6087236404418945\nDocument: FT921-4603 Distance: 0.606631338596344\nwho will become to chief execute London-quoted\nFT923-14206\nDocument: FT943-12358 Distance: 0.4833541810512543\nDocument: FT943-10833 Distance: 0.4640970826148987\nDocument: FT931-10926 Distance: 0.4627404510974884\nDocument: FT932-3576 Distance: 0.460997611284256\nDocument: FT934-11872 Distance: 0.4602893590927124\nhow many people expected to on careers services at 92 march\nFT923-14215\nDocument: FT944-9616 Distance: 0.6919786334037781\nDocument: FT921-5134 Distance: 0.6903480291366577\nDocument: FT933-11508 Distance: 0.6886097192764282\nDocument: FT941-3236 Distance: 0.6850579977035522\nDocument: FT923-9092 Distance: 0.6844646334648132\nHow does Lawler define subsidiarity, and how is it applied in high-involvement organizations\nFT924-12138\nDocument: FT921-353 Distance: 0.6311310529708862\nDocument: FT931-1040 Distance: 0.6296626925468445\nDocument: FT924-14146 Distance: 0.6256711483001709\nDocument: FT922-4194 Distance: 0.6247732043266296\nDocument: FT941-3581 Distance: 0.6235876679420471\nWhat alternative solution does M. C. Kennedy propose to manage the fiscal situation without increasing taxes\nFT931-7937\nDocument: FT932-3564 Distance: 0.7182577252388\nDocument: FT934-5855 Distance: 0.7118852734565735\nDocument: FT921-2875 Distance: 0.7107628583908081\nDocument: FT924-8520 Distance: 0.7103546857833862\nDocument: FT931-8549 Distance: 0.7097631692886353\nHow do international bond funds differ from gilt unit trusts in terms of risks and returns\nFT931-8107\nDocument: FT923-12302 Distance: 0.6297315955162048\nDocument: FT932-14906 Distance: 0.6240730881690979\nDocument: FT934-11069 Distance: 0.6061278581619263\nDocument: FT931-13185 Distance: 0.6007645726203918\nDocument: FT934-16723 Distance: 0.5994732975959778\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  }
 ]
}
