{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9982294,"sourceType":"datasetVersion","datasetId":6142619},{"sourceId":10209807,"sourceType":"datasetVersion","datasetId":6310116}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rank_bm25\n!pip install pytrec_eval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T21:27:07.719110Z","iopub.execute_input":"2025-01-16T21:27:07.719429Z","iopub.status.idle":"2025-01-16T21:27:30.680567Z","shell.execute_reply.started":"2025-01-16T21:27:07.719404Z","shell.execute_reply":"2025-01-16T21:27:30.679438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nfrom rank_bm25 import BM25Okapi\nimport pytrec_eval\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n# Download required NLTK data\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T21:27:42.680496Z","iopub.execute_input":"2025-01-16T21:27:42.680909Z","iopub.status.idle":"2025-01-16T21:27:44.412948Z","shell.execute_reply.started":"2025-01-16T21:27:42.680861Z","shell.execute_reply":"2025-01-16T21:27:44.411924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and prepare dataset\npath_to_json = '/kaggle/input/financial-times/merged_output.json'\nwith open(path_to_json, 'r') as file:\n    data = json.load(file)\n\n# Extract DOCNO and TEXT\nextracted_data = [\n    {\"DOCNO\": entry[\"DOCNO\"], \"TEXT\": entry[\"TEXT\"]}\n    for entry in data\n]\n\n# Prepare documents for BM25\ndocuments = [doc[\"TEXT\"] for doc in extracted_data]\ndocnos = [doc[\"DOCNO\"] for doc in extracted_data]\n\n# Tokenize documents\ntokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n\n# Create BM25 index\nbm25 = BM25Okapi(tokenized_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T21:28:29.841090Z","iopub.execute_input":"2025-01-16T21:28:29.841663Z","iopub.status.idle":"2025-01-16T21:39:03.357305Z","shell.execute_reply.started":"2025-01-16T21:28:29.841630Z","shell.execute_reply":"2025-01-16T21:39:03.356447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_queries(queries_file):\n    \"\"\"Load queries from JSON file\"\"\"\n    with open(queries_file, 'r') as f:\n        queries_data = json.load(f)\n    \n    query_texts = [query.get('text', '') for query in queries_data]\n    query_ids = [query.get('id', str(idx)) for idx, query in enumerate(queries_data)]\n    return query_texts, query_ids\n\ndef load_qrels(qrels_path):\n    \"\"\"Load relevance judgments\"\"\"\n    qrels = {}\n    with open(qrels_path, 'r') as f:\n        for line in f:\n            query_id, _, doc_id, relevance = line.strip().split()\n            if query_id not in qrels:\n                qrels[query_id] = {}\n            qrels[query_id][doc_id] = int(relevance)\n    return qrels\n\ndef retrieve_bm25(query_texts, bm25_model, docnos, top_k=10):\n    \"\"\"Retrieve documents using BM25\"\"\"\n    results = {}\n    \n    for idx, query in enumerate(query_texts):\n        # Tokenize query\n        tokenized_query = word_tokenize(query.lower())\n        \n        # Get BM25 scores\n        scores = bm25_model.get_scores(tokenized_query)\n        \n        # Get top-k documents\n        top_indices = np.argsort(scores)[::-1][:top_k]\n        \n        # Store results\n        results[str(idx)] = {\n            docnos[i]: float(scores[i]) \n            for i in top_indices\n        }\n    \n    return results\n\ndef compute_aggregated_measures(metrics):\n    \"\"\"Compute aggregated evaluation metrics\"\"\"\n    aggregated_metrics = {}\n    \n    metric_keys = [\n        'ndcg', 'map', 'recip_rank', \n        'P_5', 'P_10',\n        'recall_5', 'recall_10',\n    ]\n    \n    for metric in metric_keys:\n        metric_values = []\n        for query_metrics in metrics.values():\n            if metric in query_metrics:\n                metric_values.append(query_metrics[metric])\n        \n        if metric_values:\n            aggregated_metrics[f'{metric}_mean'] = np.mean(metric_values)\n            aggregated_metrics[f'{metric}_median'] = np.median(metric_values)\n            aggregated_metrics[f'{metric}_std'] = np.std(metric_values)\n    \n    return aggregated_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T21:47:06.959555Z","iopub.execute_input":"2025-01-16T21:47:06.959924Z","iopub.status.idle":"2025-01-16T21:47:06.969440Z","shell.execute_reply.started":"2025-01-16T21:47:06.959894Z","shell.execute_reply":"2025-01-16T21:47:06.968432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load queries and qrels\nquery_texts, query_ids = load_queries('/kaggle/input/query-and-qrels/queries.json')\nqrels = load_qrels('/kaggle/input/query-and-qrels/filtered_data.txt')\n\n# Retrieve results using BM25\nrun = retrieve_bm25(query_texts, bm25, docnos)\n\n# Map query indices to query IDs\nrun_with_query_ids = {\n    query_ids[int(k)]: v for k, v in run.items()\n}\n\n# Prepare for evaluation\ncorrected_version = {\n    str(query_id): {str(doc_id): float(score) for doc_id, score in doc_scores.items()}\n    for query_id, doc_scores in run_with_query_ids.items()\n}\n\n# Initialize evaluator\nevaluator = pytrec_eval.RelevanceEvaluator(\n    qrels,\n    {\n        'ndcg', 'map', 'recip_rank',\n        'P_5', 'P_10',\n        'recall_5', 'recall_10',\n    }\n)\n\n# Evaluate\nmetrics = evaluator.evaluate(corrected_version)\n\n# Compute and display aggregated metrics\nprint(\"BM25 Evaluation Results:\")\naggregated_measures = compute_aggregated_measures(metrics)\nfor metric, value in sorted(aggregated_measures.items()):\n    print(f\"{metric}: {value}\")\n\n# Save results\nresults_df = pd.DataFrame([aggregated_measures])\nresults_df.to_csv('bm25_evaluation_results.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T21:47:10.083336Z","iopub.execute_input":"2025-01-16T21:47:10.083687Z","iopub.status.idle":"2025-01-16T21:47:54.939230Z","shell.execute_reply.started":"2025-01-16T21:47:10.083660Z","shell.execute_reply":"2025-01-16T21:47:54.938188Z"}},"outputs":[],"execution_count":null}]}