{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T22:50:55.122375Z",
     "iopub.status.busy": "2025-01-16T22:50:55.122058Z",
     "iopub.status.idle": "2025-01-16T22:51:03.042027Z",
     "shell.execute_reply": "2025-01-16T22:51:03.041037Z",
     "shell.execute_reply.started": "2025-01-16T22:50:55.122349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T22:52:06.684609Z",
     "iopub.status.busy": "2025-01-16T22:52:06.684326Z",
     "iopub.status.idle": "2025-01-16T22:52:06.688506Z",
     "shell.execute_reply": "2025-01-16T22:52:06.687546Z",
     "shell.execute_reply.started": "2025-01-16T22:52:06.684587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import os\n",
    "import pytrec_eval\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T22:52:13.912080Z",
     "iopub.status.busy": "2025-01-16T22:52:13.911728Z",
     "iopub.status.idle": "2025-01-16T22:52:39.741605Z",
     "shell.execute_reply": "2025-01-16T22:52:39.740732Z",
     "shell.execute_reply.started": "2025-01-16T22:52:13.912051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load model and tokenizer from Kaggle input directory\n",
    "model_path = \"/kaggle/input/llama-3.2/transformers/1b-instruct/1\"  # Replace with your actual dataset name and model directory\n",
    "\n",
    "# Load the tokenizer and model with legacy=False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "# Set padding token to be the same as EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModel.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "model.to(device)  # Move the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T22:52:39.743285Z",
     "iopub.status.busy": "2025-01-16T22:52:39.742750Z",
     "iopub.status.idle": "2025-01-16T22:52:39.747766Z",
     "shell.execute_reply": "2025-01-16T22:52:39.746799Z",
     "shell.execute_reply.started": "2025-01-16T22:52:39.743261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_document(document):\n",
    "    inputs = tokenizer(str(document), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        # Get model outputs for the single document\n",
    "        outputs = model(**inputs)\n",
    "        # Extract last hidden states\n",
    "        embeddings = outputs.last_hidden_state[0][-1]\n",
    "        \n",
    "        #embeddings /= embeddings.norm()  # Normalize\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear cache\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T22:52:39.749155Z",
     "iopub.status.busy": "2025-01-16T22:52:39.748884Z",
     "iopub.status.idle": "2025-01-16T22:52:45.900840Z",
     "shell.execute_reply": "2025-01-16T22:52:45.899890Z",
     "shell.execute_reply.started": "2025-01-16T22:52:39.749134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# original dataset was loaded to output target document after the seach\n",
    "\n",
    "# Path to your input JSON file\n",
    "path_to_json = '/kaggle/input/financial-times/merged_output.json'\n",
    "\n",
    "# Load your JSON data from a file\n",
    "with open(path_to_json, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a dictionary where DOCNO is the key and TEXT is the value\n",
    "documents = {}\n",
    "for entry in data:\n",
    "    key = entry[\"DOCNO\"]\n",
    "    value = entry[\"TEXT\"]\n",
    "    documents[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-16T23:01:16.047456Z",
     "iopub.status.busy": "2025-01-16T23:01:16.047136Z",
     "iopub.status.idle": "2025-01-16T23:03:50.326315Z",
     "shell.execute_reply": "2025-01-16T23:03:50.325346Z",
     "shell.execute_reply.started": "2025-01-16T23:01:16.047432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dense vectors from CSV file\n",
    "def load_vectors_from_csv(file_path):\n",
    "    ids = []\n",
    "    vectors = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip the header row if it exists\n",
    "        for row in reader:\n",
    "            ids.append(row[0])  # Assuming the first column is IDs\n",
    "            vectors.append([float(value) for value in row[1:]])  # Remaining columns are vector values\n",
    "    vectors = np.array(vectors, dtype=np.float32)\n",
    "    return ids, vectors\n",
    "\n",
    "# Load dense vectors\n",
    "ids, doc_vectors = load_vectors_from_csv('/kaggle/input/llama3-2-1b-instruct-embeddings/llama3_2_1b_instruct__fin_times_embeddings.csv')\n",
    "\n",
    "# Move document vectors to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "doc_vectors_tensor = torch.tensor(doc_vectors).to(device)\n",
    "\n",
    "# Create FAISS index for efficient similarity search (cosine similarity)\n",
    "index = faiss.IndexFlatIP(doc_vectors_tensor.shape[1])  # documents were already normalized (normalization is not required)\n",
    "# Add vectors to the index\n",
    "index.add(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T22:56:41.328871Z",
     "iopub.status.busy": "2025-01-16T22:56:41.328532Z",
     "iopub.status.idle": "2025-01-16T22:56:41.334392Z",
     "shell.execute_reply": "2025-01-16T22:56:41.333527Z",
     "shell.execute_reply.started": "2025-01-16T22:56:41.328840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retrieve(query_embeddings, top_k=10):\n",
    "        \"\"\"\n",
    "        Retrieve top-k most similar documents for multiple queries\n",
    "        \n",
    "        :param query_embeddings: Numpy array of query embeddings\n",
    "        :param top_k: Number of documents to retrieve\n",
    "        :return: Dictionary of results for pytrec_eval\n",
    "        \"\"\"\n",
    "        # Ensure query embeddings are 2D\n",
    "        if query_embeddings.ndim == 1:\n",
    "            query_embeddings = query_embeddings.reshape(1, -1)\n",
    "        \n",
    "        # Search index\n",
    "        distances, indices = index.search(query_embeddings, top_k)\n",
    "        \n",
    "        # Convert results to dictionary format for pytrec_eval\n",
    "        results = {}\n",
    "        for i, (doc_indices, doc_distances) in enumerate(zip(indices, distances)):\n",
    "            # Use query index as string key\n",
    "            query_key = str(i)\n",
    "            results[query_key] = {\n",
    "                #ids[idx]: float(1 / (1 + dist)) \n",
    "                ids[idx]: dist\n",
    "                for idx, dist in zip(doc_indices, doc_distances) \n",
    "                if idx != -1\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "def load_qrels(qrels_path):\n",
    "    \"\"\"\n",
    "    Load relevance judgments from TREC qrels file\n",
    "    \n",
    "    :param qrels_path: Path to qrels file\n",
    "    :return: Dictionary of relevance judgments\n",
    "    \"\"\"\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Typical TREC qrels format: query_id 0 doc_id relevance\n",
    "            query_id, _, doc_id, relevance = line.strip().split()\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    return qrels\n",
    "\n",
    "def load_queries(queries_file):\n",
    "    \"\"\"\n",
    "    Load queries from a file\n",
    "    \n",
    "    :param queries_file: Path to queries JSON file\n",
    "    :return: Tuple of (query_texts, query_ids)\n",
    "    \"\"\"\n",
    "    with open(queries_file, 'r') as f:\n",
    "        queries_data = json.load(f)\n",
    "    \n",
    "    # Assuming JSON structure with 'text' and 'id' fields\n",
    "    query_texts = [query.get('text', '') for query in queries_data]\n",
    "    query_ids = [query.get('id', str(idx)) for idx, query in enumerate(queries_data)]\n",
    "\n",
    "    return query_texts, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T22:56:56.743736Z",
     "iopub.status.busy": "2025-01-16T22:56:56.743429Z",
     "iopub.status.idle": "2025-01-16T22:56:56.748864Z",
     "shell.execute_reply": "2025-01-16T22:56:56.747977Z",
     "shell.execute_reply.started": "2025-01-16T22:56:56.743714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_aggregated_measures(metrics):\n",
    "    \"\"\"\n",
    "    Manually compute aggregated measures across queries\n",
    "    \n",
    "    :param metrics: Dictionary of per-query metrics from pytrec_eval\n",
    "    :return: Dictionary of aggregated metrics\n",
    "    \"\"\"\n",
    "    aggregated_metrics = {}\n",
    "    \n",
    "    # Metrics to aggregate\n",
    "    metric_keys = [\n",
    "        'ndcg', 'map', 'recip_rank', \n",
    "        'P_5', 'P_10',\n",
    "        'recall_5', 'recall_10', \n",
    "    ]\n",
    "    \n",
    "    for metric in metric_keys:\n",
    "        # Collect all values for this metric\n",
    "        metric_values = []\n",
    "        for query_metrics in metrics.values():\n",
    "            if metric in query_metrics:\n",
    "                metric_values.append(query_metrics[metric])\n",
    "        \n",
    "        # Compute aggregation methods\n",
    "        if metric_values:\n",
    "            aggregated_metrics[f'{metric}_mean'] = np.mean(metric_values)\n",
    "            aggregated_metrics[f'{metric}_median'] = np.median(metric_values)\n",
    "            aggregated_metrics[f'{metric}_std'] = np.std(metric_values)\n",
    "    \n",
    "    return aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:05:49.627121Z",
     "iopub.status.busy": "2025-01-16T23:05:49.626754Z",
     "iopub.status.idle": "2025-01-16T23:05:53.406611Z",
     "shell.execute_reply": "2025-01-16T23:05:53.405878Z",
     "shell.execute_reply.started": "2025-01-16T23:05:49.627091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "query_texts, query_ids = load_queries('/kaggle/input/query-and-qrels/queries.json')\n",
    "\n",
    "qrels = load_qrels('/kaggle/input/query-and-qrels/filtered_data.txt')\n",
    "\n",
    "query_embeddings = []\n",
    "for query in query_texts:\n",
    "    query_embeddings.append(encode_document(query))\n",
    "\n",
    "\n",
    "query_embeddings = np.array(query_embeddings, dtype=np.float32)\n",
    "\n",
    "\n",
    "run = retrieve(query_embeddings)\n",
    "\n",
    "#print(run)\n",
    "\n",
    "run_with_query_ids = {\n",
    "        query_ids[int(k)]: v for k, v in run.items()\n",
    "    }\n",
    "\n",
    "\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrels, \n",
    "        {\n",
    "            'ndcg', 'map', 'recip_rank', \n",
    "            'P_5', 'P_10', \n",
    "            'recall_5', 'recall_10',\n",
    "        }\n",
    "    )\n",
    "\n",
    "corrected_version = {\n",
    "    str(query_id): {str(doc_id): float(score) for doc_id, score in doc_scores.items()}\n",
    "    for query_id, doc_scores in run_with_query_ids.items()\n",
    "}\n",
    "\n",
    "#print(run_with_query_ids)\n",
    "\n",
    "metrics = evaluator.evaluate(corrected_version)\n",
    "\n",
    "print(\"Aggregated Metrics:\")\n",
    "aggregated_measures = compute_aggregated_measures(\n",
    "        metrics\n",
    "    )\n",
    "\n",
    "for metric, value in sorted(aggregated_measures.items()):\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:16:16.125941Z",
     "iopub.status.busy": "2025-01-16T23:16:16.125580Z",
     "iopub.status.idle": "2025-01-16T23:16:16.132849Z",
     "shell.execute_reply": "2025-01-16T23:16:16.132200Z",
     "shell.execute_reply.started": "2025-01-16T23:16:16.125885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_metrics_to_csv(metrics, aggregated_measures, model_name=\"llama3_2_1b_instruct\", base_path=\"/kaggle/working\", mode='w'):\n",
    "    # Save aggregated metrics with model name as row\n",
    "    aggregated_metrics_path = f\"{base_path}/model_metrics.csv\"\n",
    "    file_exists = os.path.exists(aggregated_metrics_path)\n",
    "    \n",
    "    try:\n",
    "        with open(aggregated_metrics_path, mode, newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Write header only if file is new or in write mode\n",
    "            if mode == 'w' or not file_exists:\n",
    "                headers = ['model_name'] + sorted(aggregated_measures.keys())\n",
    "                writer.writerow(headers)\n",
    "            # Write values\n",
    "            row = [model_name] + [aggregated_measures[metric] for metric in sorted(aggregated_measures.keys())]\n",
    "            writer.writerow(row)\n",
    "        print(f\"Metrics saved to: {aggregated_metrics_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving metrics: {e}\")\n",
    "\n",
    "# Usage example:\n",
    "save_metrics_to_csv(metrics, aggregated_measures, model_name=\"llama3_2_1b_instruct\", mode='a')  # Use 'a' to append to existing file\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6142619,
     "sourceId": 9982294,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6239976,
     "sourceId": 10113929,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6310116,
     "sourceId": 10209807,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6330476,
     "sourceId": 10237289,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 121027,
     "modelInstanceId": 100933,
     "sourceId": 120002,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
